{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i will be using dataset downloaded from https://archive.ics.uci.edu/ml/index.php. It's a great public repository of datasets to use for machine learning projects. The dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "train = pd.read_csv('drugsCom_raw/drugsComTrain_raw.tsv', delimiter=\"\\t\")\n",
    "test = pd.read_csv('drugsCom_raw/drugsComTest_raw.tsv', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few records of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  drugName                     condition  \\\n",
       "0      206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260                Guanfacine                          ADHD   \n",
       "2       92703                    Lybrel                 Birth Control   \n",
       "3      138000                Ortho Evra                 Birth Control   \n",
       "4       35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "3  \"This is my first time using any form of birth...     8.0   \n",
       "4  \"Suboxone has completely turned my life around...     9.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  \n",
       "3   November 3, 2015           10  \n",
       "4  November 27, 2016           37  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings distribution shows higher proportion of 8, 9 and 10 stars reviews as well as 1 star. Everything else in between is less frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1.0     21619\n",
       "2.0      6931\n",
       "3.0      6513\n",
       "4.0      5012\n",
       "5.0      8013\n",
       "6.0      6343\n",
       "7.0      9456\n",
       "8.0     18890\n",
       "9.0     27531\n",
       "10.0    50989\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('rating')['rating'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of natural language processing algorithms is to represent text as feature vectors. A unique feature will be assigned for every word and number of occurences will be counted for each word in each review. First let's do some text preprocessing - remove non letters, switch to lower case, remove one letter words, remove stop words and apply \"stemming\" technique.\n",
    "\n",
    "Stop words such as \"and\", \"but\", etc don't add any valuable information. NLTK library (natural language toolkit) - https://www.nltk.org/ provides a list of stopwords which i am going to compare against.\n",
    "\n",
    "\"Stemming\" is a techinque of converting a word to its root for example \"going\", \"went\" will be converted to \"go\", etc. This allows to reduce dimensionality of the data while preserving valuable information. NLTK library provides that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kostya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def t_transform(review):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    return [ps.stem(word) for word in review if len(word) > 1 and word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the reviews would look like after applying all the preprocessing steps described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [side, effect, take, combin, bystol, mg, fish,...\n",
       "1    [son, halfway, fourth, week, intuniv, becam, c...\n",
       "2    [use, take, anoth, oral, contracept, pill, cyc...\n",
       "3    [first, time, use, form, birth, control, glad,...\n",
       "4    [suboxon, complet, turn, life, around, feel, h...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].head(5).apply(t_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to see whether the NLP would be able to differentiate between positive and negative reviews. So anything above 6 stars would be considered positive and anything below 5 starts negative. I am going to label all reviews this way and remove 5 and 6 stars reviews from both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train['rating'] <= 4) | (train['rating'] >= 7)]\n",
    "train['rating2'] = [0 if x <= 4 else 1 for x in train['rating']]\n",
    "test = test[(test['rating'] <= 4) | (test['rating'] >= 7)]\n",
    "test['rating2'] = [0 if x <= 4 else 1 for x in test['rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all necessary tools for NLP analysis from sklearn.\n",
    "\n",
    "CountVectorizer creates a separate feature for each word in the document and counts word occurrences for each review. So every review will be represented by a very long vector with most features as zeros. This model is called bag of words.\n",
    "\n",
    "TfidfTransformer implements technique called term frequency - inverse document frequency. Instead of just counting word frequencies as in the step above, TFIDF will assign a weight. The weight is based on a product of TF*IDF where TF is a word frequency relative to length of the review and IDF depends on how often the word appears in other reviews and assigns more weight to less frequent ones:\n",
    "\n",
    "TF(t) = (Number of word occurences in a review) / (Total number of words in the review)\n",
    "\n",
    "IDF(t) = log_e(Total number of reviews / Number of reviews with such word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline allows to assemble several steps and supply parameters for them. I will sequentially implement vectorizing, TFIDF and fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', CountVectorizer()),  \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('clf', MultinomialNB()),  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be provided by a step name following a double underscore. \"vec__analyzer\" is supplying test preprocessing function that i already created before. \"vec__ngram_range\" specifies whether contiguous sequences of words should be taken into account. For example \"vec__ngram_range\" = [(1,2)] would look at both 1-word and 2-words combinations. \"vec__max_df\" will ignore words that appear in more than X% of reviews. \"clf__alpha\" is a smoothing parameter for multinomial naive bayes classifier - MultinomialNB. More on this https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = dict(vec__analyzer=[t_transform],\n",
    "                    vec__ngram_range=[(1, 1), (1, 2)],\n",
    "                    vec__max_df=[0.5, 0.75],\n",
    "                    clf__alpha=(1e-2, 1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV is used here to find the best combination of hyper-parameters. By default 3-fold cross validation is used as well. Although 3 folds is a little low number for cross validation to make sure there is no overfitting. I already trained this model with higher number of folds and found no overfitting so the default is used.\n",
    "\n",
    "GridSearchCV is supplied with pipe object and parameters dictionary created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid=tuned_params,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model and see what results we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 519.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  32720.129016160965\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "grid.fit(train['review'], train['rating2'])\n",
    "print(\"Time spent: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters combination is shown below. There were 8 combinations in total and average accuracy is shown for each combination. They are pretty close. Accuracy metric is just number of reviews where target variable was correctly predicted by the model over total number of reviews. Accuracy is around 81%.\n",
    "\n",
    "GridSearchCV would train a model with each hyper-parameters combination using k-fold technique and select the best combination based on \"scoring\" parameter (in this case accuracy). Then the whole train dataset will be used to re-train the model again with the best hyper-parameters combination just found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'clf__alpha': 0.01, 'vec__analyzer': <function t_transform at 0x7f632a344ea0>, 'vec__max_df': 0.5, 'vec__ngram_range': (1, 1)}\n",
      "Best score:  0.8103864816490972\n",
      "[0.81038648 0.81038648 0.81038648 0.81038648 0.80996454 0.80996454\n",
      " 0.80996454 0.80996454]\n",
      "[0.0006044  0.0006044  0.0006044  0.0006044  0.00050035 0.00050035\n",
      " 0.00050035 0.00050035]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "print(grid.cv_results_['mean_test_score'])\n",
    "print(grid.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can test the model on test dataset. The classification report and confusion matrix provide important information about the model performance - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\n",
    "\n",
    "The model is very good identifying \"positive\" reviews based on recall metric - 98% of positive reviews were predicted correctly (target variable 1 - positive review). At the same time only 39% of negative reviews were guessed right by the model. Could it be cause 3 and 4 stars reviews are more difficult to predict? It would be interesting to see whether the model can predict 1 and 2 starts reviews better than 1, 2, 3 and 4 stars.\n",
    "\n",
    "Consusion matrix below just shows number of false positive, false negative, true positive and true negative. I can calculate accuracy based on this: (5228+34618)/48937 = 81.4% which is in line with accuracy obtained during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.39      0.53     13497\n",
      "          1       0.81      0.98      0.88     35440\n",
      "\n",
      "avg / total       0.82      0.81      0.79     48937\n",
      "\n",
      "[[ 5228  8269]\n",
      " [  822 34618]]\n"
     ]
    }
   ],
   "source": [
    "pred = grid.predict(test['review'])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(test['rating2'], pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test['rating2'], pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
